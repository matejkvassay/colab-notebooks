{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GQ1i6BJhSbBH",
        "FTk0newzEYHF",
        "28bRewMwSWW_"
      ],
      "authorship_tag": "ABX9TyOmuh8yS/Apr7kfRefBYJiH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matejkvassay/colab-notebooks/blob/master/bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNOYo-PUBEpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q tensorflow\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ1i6BJhSbBH",
        "colab_type": "text"
      },
      "source": [
        "# 1 - Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C3jynU0BOJs",
        "colab_type": "text"
      },
      "source": [
        "## Prepare data pipeline "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxexDUzCBGbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
        "                               as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60d5JK1bLC-I",
        "colab_type": "text"
      },
      "source": [
        "####Inspect\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UStbWvtxEgMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "967d07c4-d4c4-4d88-a24e-9bf8202b7288"
      },
      "source": [
        "pt_ex, en_ex = train_examples.take(1).__iter__().__next__()\n",
        "print('PT example: {}'.format(pt_ex))\n",
        "print('EN example: {}'.format(en_ex))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PT example: b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .'\n",
            "EN example: b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiOV9uZwCrAl",
        "colab_type": "text"
      },
      "source": [
        "## Train tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyWFAfs8Ckp-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8b16ceda-8be1-468c-9960-e496773d0bde"
      },
      "source": [
        "%%time\n",
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
        "\n",
        "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 35s, sys: 7.75 s, total: 2min 43s\n",
            "Wall time: 2min 26s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82BBNr4JFhMj",
        "colab_type": "text"
      },
      "source": [
        "#### Inspect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otuiazz-Fft5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "266b6327-b1b3-41fb-c93b-8c3b723a4198"
      },
      "source": [
        "sample_string = en_ex.numpy().decode('utf8')\n",
        "print('Sent:\\n{}'.format(sample_string))\n",
        "\n",
        "tokenized_string = tokenizer_en.encode(sample_string)\n",
        "print('Token IDs:\\n{}'.format(tokenized_string))\n",
        "\n",
        "for token_id in tokenized_string:\n",
        "  print ('{} ----> {}'.format(token_id, tokenizer_en.decode([token_id])))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sent:\n",
            "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
            "Token IDs:\n",
            "[4, 59, 15, 1792, 6561, 3060, 7952, 1, 15, 103, 134, 378, 3, 47, 6122, 6, 5311, 1, 91, 13, 1849, 559, 1609, 894, 2]\n",
            "4 ----> and \n",
            "59 ----> when \n",
            "15 ----> you \n",
            "1792 ----> improve \n",
            "6561 ----> search\n",
            "3060 ----> abilit\n",
            "7952 ----> y\n",
            "1 ---->  , \n",
            "15 ----> you \n",
            "103 ----> actually \n",
            "134 ----> take \n",
            "378 ----> away \n",
            "3 ----> the \n",
            "47 ----> one \n",
            "6122 ----> advantage \n",
            "6 ----> of \n",
            "5311 ----> print\n",
            "1 ---->  , \n",
            "91 ----> which \n",
            "13 ----> is \n",
            "1849 ----> ser\n",
            "559 ----> end\n",
            "1609 ----> ip\n",
            "894 ----> ity\n",
            "2 ---->  .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTk0newzEYHF",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess data\n",
        "- add start & end tokens \n",
        "- filter out examples with >40 words\n",
        "- prepare batches (pad shorter to maximum batch size)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4087zzxDCuHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE=200000\n",
        "MAX_LEN=40\n",
        "BATCH_SIZE=64\n",
        "\n",
        "def encode(lang1, lang2):\n",
        "  # start token idx = vocab size\n",
        "  # end token idx = vocab size + 1\n",
        "  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
        "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
        "\n",
        "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
        "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
        "  \n",
        "  return lang1, lang2\n",
        "\n",
        "def tf_encode(pt, en):\n",
        "  # make it work in graph (lazy execution)\n",
        "  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
        "  result_pt.set_shape([None])\n",
        "  result_en.set_shape([None])\n",
        "\n",
        "  return result_pt, result_en\n",
        "\n",
        "def filter_max_length(x, y, max_length=MAX_LEN):\n",
        "  # drop examples with more than max_length words\n",
        "  return tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y) <= max_length)\n",
        "  \n",
        "train_preprocessed = (\n",
        "    train_examples\n",
        "    .map(tf_encode) \n",
        "    .filter(filter_max_length)\n",
        "    # cache the dataset to memory to get a speedup while reading from it.\n",
        "    .cache()\n",
        "    .shuffle(BUFFER_SIZE))\n",
        "\n",
        "val_preprocessed = (\n",
        "    val_examples\n",
        "    .map(tf_encode)\n",
        "    .filter(filter_max_length))  \n",
        "\n",
        "# pad & prepare batches    \n",
        "train_dataset = (train_preprocessed\n",
        "                 .padded_batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "\n",
        "val_dataset = (val_preprocessed.padded_batch(BATCH_SIZE))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3R18hu6R08v",
        "colab_type": "text"
      },
      "source": [
        "#### Inspect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSlmb865RM5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "fb7138a2-bd1d-42a3-a5b1-e56249bff4c6"
      },
      "source": [
        "# will take time - catches train set\n",
        "pt_batch, en_batch = next(iter(train_dataset))\n",
        "pt_batch, en_batch"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(64, 37), dtype=int64, numpy=\n",
              " array([[8214,   42,   13, ...,    0,    0,    0],\n",
              "        [8214,  270, 4880, ...,    0,    0,    0],\n",
              "        [8214,  278,    5, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [8214,    3, 2200, ...,    0,    0,    0],\n",
              "        [8214,   25,  422, ...,    0,    0,    0],\n",
              "        [8214,  164, 4010, ...,    0,    0,    0]])>,\n",
              " <tf.Tensor: shape=(64, 37), dtype=int64, numpy=\n",
              " array([[8087,    4,   16, ...,    0,    0,    0],\n",
              "        [8087,   14,  124, ...,    0,    0,    0],\n",
              "        [8087,   12,   84, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [8087,    3,  132, ...,    0,    0,    0],\n",
              "        [8087,   15,  101, ...,    0,    0,    0],\n",
              "        [8087,   12,   24, ...,    0,    0,    0]])>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28bRewMwSWW_",
        "colab_type": "text"
      },
      "source": [
        "#2 - Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvBfv60TS3IL",
        "colab_type": "text"
      },
      "source": [
        "## Positional encoding\n",
        "https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEpvx1tYS6pA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}