{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_is_all_you_need.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMiuYnQYiPnFwVOYbZom5SH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matejkvassay/colab-notebooks/blob/master/attention_is_all_you_need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5KBqho1j_-n"
      },
      "source": [
        "MINUS_INF=float(\"-1e15\")\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, emb_size, n_heads):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self._check_input_params(emb_size, n_heads)\n",
        "\n",
        "    self.emb_size = emb_size\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = emb_size // n_heads\n",
        "    self.keys, self.queries, self.values = self._init_att_matrices(self.head_dim)\n",
        "    self.fc_out = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "  def forward(self, keys, queries, values, mask=None):\n",
        "    keys, queries, values = self._split_input_embeddings(keys, queries, values)\n",
        "    keys, queries, values = self._send_to_linear_layers(keys, queries, values)\n",
        "    attention = self._compute_masked_attention(queries, keys, mask)\n",
        "    result = self._multiply_with_values(attention, queries, values)\n",
        "    return self.fc_out(result)\n",
        "\n",
        "  def _send_to_linear_layers(self, keys, queries, values):\n",
        "    keys = self.keys(keys)\n",
        "    queries = self.queries(queries)\n",
        "    values = self.values(values)\n",
        "    return keys, queries, values\n",
        "\n",
        "  def _compute_masked_attention(self, queries, keys, mask):\n",
        "    '''\n",
        "    1. nqhd = queries shape (N, query_len, n_heads, heads_dim)\n",
        "    2. nkhd = keys shape: (N, key_len, heads, heads_dim)\n",
        "    '''\n",
        "    comparison = torch.einsum(\"nqhd, nkhd->nhqk\", [queries, keys])\n",
        "    if mask is not None:\n",
        "      comparison=comparison.masked_fill(mask == 0, MINUS_INF)\n",
        "    att = torch.softmax(comparison / (self.emb_size**(0.5)), dim=3)\n",
        "    return att  \n",
        "\n",
        "  def _multiply_with_values(self, attention, queries, values):\n",
        "    '''\n",
        "    1. attention shape: (N, heads, query_len, key_len)\n",
        "    2. values_shape: (N, value_len, n_heads, heads_dim)\n",
        "    l = key_len == val_len => placeholder l\n",
        "    '''\n",
        "    output = torch.einsum('nhql, nlhd->nqhd', [attention, values])\n",
        "    return output.reshape(queries.shape[0], queries.shape[1], self.emb_size)\n",
        "\n",
        "  def _split_input_embeddings(self, keys, queries, values):\n",
        "    n_samples = queries.shape[0]\n",
        "    input = (keys, queries, values)\n",
        "    output = tuple(mtx.reshape(n_samples, mtx.shape[1], self.n_heads, self.head_dim) \\\n",
        "                 for i, mtx in enumerate(input))\n",
        "    return output\n",
        "\n",
        "  @staticmethod\n",
        "  def _check_input_params(emb_size, n_heads):\n",
        "    if emb_size % n_heads != 0:\n",
        "      raise ValueError(f'emb_size must be divisible by n_heads, values given: \\\n",
        "      emb_size: {emb_size}, n_heads: {n_heads}')\n",
        "\n",
        "  def _init_att_matrices(self, head_dim):\n",
        "    return (nn.Linear(self.head_dim, self.head_dim, bias=False) for _ in range(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rly4E_XR0Xob"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, emb_size, n_heads, dropout, expansion):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.attention = SelfAttention(emb_size, n_heads)\n",
        "    self.norm_before = nn.LayerNorm(emb_size)\n",
        "    self.norm_after = nn.LayerNorm(emb_size)\n",
        "    self.feed_fwd = nn.Sequential(\n",
        "        nn.Linear(emb_size, expansion*emb_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(expansion*emb_size, emb_size)\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, values, keys, queries, mask=None):\n",
        "    attention = self.attention(keys, queries, values, mask=mask)\n",
        "    x = self.norm_before(attention + queries)\n",
        "    x = self.dropout(x)\n",
        "    forward = self.feed_fwd(x) + x\n",
        "    forward = self.norm_after(forward)\n",
        "    return self.dropout(forward)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCktDK6qA8mc"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_size, n_layers, n_heads, expansion, device, dropout, max_seq_len):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.emb_size = emb_size\n",
        "    self.device = device\n",
        "    self.word_emb = nn.Embedding(vocab_size, emb_size)\n",
        "    self.pos_emb = nn.Embedding(max_seq_len, emb_size)\n",
        "    self.layers = nn.ModuleList(\n",
        "        [TransformerBlock(emb_size, n_heads, dropout, expansion) \\\n",
        "         for _ in range(n_layers)]\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    embedding = self._embed(x)\n",
        "    embedding = self.dropout(embedding)\n",
        "    for layer in self.layers:\n",
        "      output = layer(embedding, embedding, embedding, mask)\n",
        "    return output \n",
        "\n",
        "  def _embed(self, x):\n",
        "    n_samples, seq_len = x.shape\n",
        "    positions = torch.arange(0, seq_len)\n",
        "    positions = positions.expand(n_samples, seq_len).to(self.device)\n",
        "    embedding = self.word_emb(x) + self.pos_emb(x)\n",
        "    return embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKh8_MTFCbzt"
      },
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, emb_size, n_heads, expansion, dropout, device):\n",
        "    super(DecoderBlock, self).__init__()\n",
        "    self.attention = SelfAttention(emb_size, n_heads)\n",
        "    self.norm = nn.LayerNorm(emb_size)\n",
        "    self.transformer_block = TransformerBlock(emb_size, n_heads, dropout, expansion)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, values, keys, target_mask, source_mask):\n",
        "    attention = self.attention(x, x, x, mask=target_mask)\n",
        "    queries = attention + x\n",
        "    queries = self.norm(attention)\n",
        "    queries = self.dropout(attention)\n",
        "    return self.transformer_block(values, keys, queries, source_mask)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYqzOCeFj6T5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyfGL_CkLu6M"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, target_vocab_size, emb_size, n_layers, n_heads, expansion, dropout, device, max_seq_len):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.device=device\n",
        "    self.word_emb = nn.Embedding(target_vocab_size, emb_size)\n",
        "    self.pos_emb = nn.Embedding(max_seq_len, emb_size)\n",
        "    self.layers = nn.ModuleList(\n",
        "        [DecoderBlock(emb_size, n_heads, expansion, dropout, device) for _ in range(n_layers)]\n",
        "    )\n",
        "    self.fc_out = nn.Linear(emb_size, target_vocab_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, enc_out, target_mask, source_mask):\n",
        "    n_samples, seq_len = x.shape\n",
        "    positions = torch.arange(0, seq_len)\n",
        "    positions = positions.expand(n_samples, seq_len).to(self.device)\n",
        "    x = self.word_emb(x) + self.pos_emb(positions)\n",
        "    x = self.dropout(x)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, enc_out, enc_out, target_mask, source_mask)\n",
        "    return self.fc_out(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZThIntV8N4yR"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,\n",
        "               tgt_vocab_size, \n",
        "               src_vocab_size, \n",
        "               tgt_pad_idx, \n",
        "               src_pad_idx, \n",
        "               emb_size=256,\n",
        "               n_layers=5, \n",
        "               expansion=4, \n",
        "               n_heads=8,\n",
        "               dropout=0,\n",
        "               device='cuda',\n",
        "               max_seq_len=100):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder = Encoder(src_vocab_size, emb_size, n_layers, n_heads, expansion, device, dropout, max_seq_len)\n",
        "    self.decoder = Decoder(tgt_vocab_size, emb_size, n_layers, n_heads, expansion, dropout, device, max_seq_len)\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.tgt_pad_idx = tgt_pad_idx\n",
        "    self.device=device\n",
        "\n",
        "  def make_src_mask(self, src):\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    return src_mask.to(self.device)\n",
        "\n",
        "\n",
        "  def make_tgt_mask(self, tgt):\n",
        "    n_samples, tgt_len = tgt.shape\n",
        "    tgt_mask = torch.tril(torch.ones((tgt_len, tgt_len)))\n",
        "    tgt_mask = tgt_mask.expand(n_samples, 1, tgt_len, tgt_len)\n",
        "    return tgt_mask.to(self.device)\n",
        "\n",
        "  def forward(self, src, tgt):\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    tgt_mask = self.make_tgt_mask(tgt)\n",
        "    enc_src = self.encoder(src, mask = src_mask)\n",
        "    return self.decoder(tgt, enc_src, tgt_mask, src_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcMOsYREP8nx",
        "outputId": "016797db-d8db-4e96-ed80-0b92822f0821"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "x = torch.tensor([[1,5,6,4,3,9,5,2,0], [1,8,7,3,4,5,6,7,2]]).to(device)\n",
        "tgt = torch.tensor([[1,7,4,3,5,9,2,0],[1,5,6,2,4,7,6,2]]).to(device)\n",
        "\n",
        "src_pad_idx = 0\n",
        "tgt_pad_idx = 0\n",
        "src_vocab_size = 10\n",
        "tgt_vocab_size = 10\n",
        "\n",
        "model = Transformer(tgt_vocab_size, src_vocab_size, tgt_pad_idx, src_pad_idx, device=device)\n",
        "model.to(device)\n",
        "output = model(x, tgt[:,:-1])\n",
        "print(output.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 7, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SYId_DcRG6c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}